<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="PhysicsML" />
    <meta name="robots" content="index, follow"/>

    <meta property="og:title" content="The Unreasonable Effectiveness of Restricted Boltzmann Machines"/>
    <meta property="og:url" content="/drafts/the-unreasonable-effectiveness-of-restricted-boltzmann-machines.html"/>
    <meta property="og:site_name" content="&lt;&nbsp;physics&nbsp;|&nbsp;machine&nbsp;learning&nbsp;&gt;"/>
    <meta property="og:type" content="article"/>

    <link rel="canonical" href="/drafts/the-unreasonable-effectiveness-of-restricted-boltzmann-machines.html" />

    <title>The Unreasonable Effectiveness of Restricted Boltzmann Machines | &lt;&nbsp;physics&nbsp;|&nbsp;machine&nbsp;learning&nbsp;&gt;</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" type="text/css" href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" />
    <link rel="stylesheet" type="text/css" href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" />

    <link rel="stylesheet" type="text/css" href="/theme/css/main.css" />

    <!--
    <script type="text/javascript">var switchTo5x=true;</script>
    <script type="text/javascript" src="http://w.sharethis.com/button/buttons.js"></script>
    <script type="text/javascript">
        stLight.options({
            publisher: "",
            doNotHash: false,
            doNotCopy: false,
            hashAddressBar: false
        });
    </script>
    -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['@@','@@'] ]}});
    </script>
    <script type="text/javascript"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
    </script>
</head>

<body id="index">
    <div class="row-fluid">
        <div class="span10 offset1">
            <header id="banner" >
                <h1>
                    <a href="/">&lt;&nbsp;physics&nbsp;|&nbsp;machine&nbsp;learning&nbsp;&gt; </a>
                </h1>
                <nav class="navbar">
                    <div class="navbar-inner">
                        <ul class="nav">

                            <li class="active"><a href="/category/articles.html">Articles</a></li>
                            <li ><a href="/category/news.html">News</a></li>

                            <li ><a href="/pages/papers.html">Papers</a></li>

                        </ul>

                    </div>
                </nav>
            </header><!-- /#banner -->
        </div>
    </div>

    <div class="row-fluid">
        <div class="span10 offset1">
            <div class="row-fluid">
<div class="span10 offset1">
  <section>
    <article>
      <header>
        <h1 class="entry-title">
          <a href="/drafts/the-unreasonable-effectiveness-of-restricted-boltzmann-machines.html" rel="bookmark"
             title="Permalink to The Unreasonable Effectiveness of Restricted Boltzmann Machines">The Unreasonable Effectiveness of Restricted Boltzmann Machines</a></h1>
      </header>
      <div class="entry-content">
<footer class="post-info">
    <address class="vcard author">
        by <a class="url fn" href="/author/roger-melko.html">Roger Melko</a>
    </address>

    in <a href="/category/articles.html">Articles</a>

    on 2016-09-10

        |
        tags:         <a href="/tag/quantum-mechanics.html">quantum mechanics</a>,        <a href="/tag/machine-learning.html">machine learning</a>,        <a href="/tag/boltzmann-machines.html">Boltzmann Machines</a>


    
</footer><!-- /.post-info -->

        <p>When I first encountered a Boltzmann Machine, I did not immediately see its appeal to a condensed matter physicist.  Why would we need a learning tool to approximate distributions, when, presumably, in order to obtain data to train the machine, we must already ''know'' the distribution (or, at least, the Hamiltonian)?  I knew as a Monte Carlo practitioner, we are already in the business of sampling from perfect distributions -- a practice fraught with many difficulties as it is.  Why would we add further systematic error (on top of our statistical error) from an imperfectly-learned distribution?  The application of such a machine in the technology industry is more obvious.  There, we are trying to train a machine on say a finite-size number of pictures of cats that is itself capable of generating (or dreaming) a picture of a ``cat''.  For me, each picture of a cat is instead (say) a spin configuration, each pixel and up or down spin state, sampled from (say) the Boltzmann distribution at finite temperature.  This sampling is done with a Markov Chain Monte Carlo procedure, an efficient algorithm honed by scientists since the end of Word-War 2.  It seems, we already have the distribution, and we already know how to sample it very efficiently.  Why would we use these samples to train an imperfect machine, the sampling of which may be less efficient than our existing algorithms, honed for the past 65 years?</p>
<p>(<a href="http://www.theverge.com/2016/6/14/11939310/andy-rubin-google-android-playground-ai-robotics" title="Andy Rubin AI Robotics">Andy Rubin</a> among them) </p>

      </div><!-- /.entry-content -->

    </article>
  </section>
</div>
            </div>
        </div>
    </div>

    <footer id="site-footer">
        <div class="row-fluid">
            <div class="span10 offset1">
                <address>
                    <p>
                        Blog powered by <a href="http://getpelican.com/">Pelican</a>, theme based on <a href="http://github.com/jsliang/pelican-fresh/">Fresh</a>.
                    </p>
                </address>
            </div>
        </div>
    </footer>

    <script src="//code.jquery.com/jquery.min.js"></script>
    <script src="//netdna.bootstrapcdn.com/bootstrap/2.3.2/js/bootstrap.min.js"></script>
</body>
</html>